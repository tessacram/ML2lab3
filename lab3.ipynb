{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29b72b5f8cb2ce33aa81c939b8d2138c",
     "grade": false,
     "grade_id": "cell-02487845739eb4fd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Lab 3: Expectation Maximization and Variational Autoencoder\n",
    "\n",
    "### Machine Learning 2 (2019)\n",
    "\n",
    "* The lab exercises can be done in groups of two people, or individually.\n",
    "* The deadline is Tuesday, October 15th at 17:00.\n",
    "* Assignment should be submitted through Canvas! Make sure to include your and your teammates' names with the submission.\n",
    "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file should be \"studentid1\\_studentid2\\_lab#\", for example, the attached file should be \"12345\\_12346\\_lab1.ipynb\". Only use underscores (\"\\_\") to connect ids, otherwise the files cannot be parsed.\n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please ask.\n",
    "* Use __one cell__ for code and markdown answers only!\n",
    "    * Put all code in the cell with the ```# YOUR CODE HERE``` comment and overwrite the ```raise NotImplementedError()``` line.\n",
    "    * For theoretical questions, put your solution using LaTeX style formatting in the YOUR ANSWER HERE cell.\n",
    "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
    "* Large parts of you notebook will be graded automatically. Therefore it is important that your notebook can be run completely without errors and within a reasonable time limit. To test your notebook before submission, select Kernel -> Restart \\& Run All.\n",
    "$\\newcommand{\\bx}{\\mathbf{x}} \\newcommand{\\bpi}{\\mathbf{\\pi}} \\newcommand{\\bmu}{\\mathbf{\\mu}} \\newcommand{\\bX}{\\mathbf{X}} \\newcommand{\\bZ}{\\mathbf{Z}} \\newcommand{\\bz}{\\mathbf{z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4e05229ee79b55d6589e1ea8de68f32",
     "grade": false,
     "grade_id": "cell-a0a6fdb7ca694bee",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Installing PyTorch\n",
    "\n",
    "In this lab we will use PyTorch. PyTorch is an open source deep learning framework primarily developed by Facebook's artificial-intelligence research group. In order to install PyTorch in your conda environment go to https://pytorch.org and select your operating system, conda, Python 3.6, no cuda. Copy the text from the \"Run this command:\" box. Now open a terminal and activate your 'ml2labs' conda environment. Paste the text and run. After the installation is done you should restart Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9c3d77f550b5fd93b34fd18825c47f0",
     "grade": false,
     "grade_id": "cell-746cac8d9a21943b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### MNIST data\n",
    "\n",
    "In this Lab we will use several methods for unsupervised learning on the MNIST dataset of written digits. The dataset contains digital images of handwritten numbers $0$ through $9$. Each image has 28x28 pixels that each take 256 values in a range from white ($= 0$) to  black ($=1$). The labels belonging to the images are also included. \n",
    "Fortunately, PyTorch comes with a MNIST data loader. The first time you run the box below it will download the MNIST data set. That can take a couple of minutes.\n",
    "The main data types in PyTorch are tensors. For Part 1, we will convert those tensors to numpy arrays. In Part 2, we will use the torch module to directly work with PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fbc152afa1255331d7b88bf00b7156c",
     "grade": false,
     "grade_id": "cell-7c995be0fda080c0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kiki/miniconda3/envs/ml2labs/lib/python3.6/site-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/Users/Kiki/miniconda3/envs/ml2labs/lib/python3.6/site-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "train_labels = train_dataset.train_labels.numpy()\n",
    "train_data = train_dataset.train_data.numpy()\n",
    "# For EM we will use flattened data\n",
    "train_data = train_data.reshape(train_data.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4fc852f9bfb0bab10d4c23eada309e89",
     "grade": false,
     "grade_id": "cell-8b4a44df532b1867",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Part 1: Expectation Maximization\n",
    "We will use the Expectation Maximization (EM) algorithm for the recognition of handwritten digits in the MNIST dataset. The images are modelled as a Bernoulli mixture model (see Bishop $\\S9.3.3$):\n",
    "$$\n",
    "p(\\bx|\\bmu, \\bpi) = \\sum_{k=1}^K  \\pi_k \\prod_{i=1}^D \\mu_{ki}^{x_i}(1-\\mu_{ki})^{(1-x_i)}\n",
    "$$\n",
    "where $x_i$ is the value of pixel $i$ in an image, $\\mu_{ki}$ represents the probability that pixel $i$ in class $k$ is black, and $\\{\\pi_1, \\ldots, \\pi_K\\}$ are the mixing coefficients of classes in the data. We want to use this data set to classify new images of handwritten numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54064637b7e7cf938c0f778d748a226a",
     "grade": false,
     "grade_id": "cell-af03fef663aa85b2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 1.1 Binary data (5 points)\n",
    "As we like to apply our Bernoulli mixture model, write a function `binarize` to convert the (flattened) MNIST data to binary images, where each pixel $x_i \\in \\{0,1\\}$, by thresholding at an appropriate level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe8607a4d734f7f26ef1ee1e54b33471",
     "grade": false,
     "grade_id": "cell-ec4365531ca57ef3",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def binarize(X):\n",
    "    return where(X > 128, 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "231b2c9f29bc5c536c60cef4d74793a1",
     "grade": true,
     "grade_id": "cell-2f16f57cb68a83b3",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test test test!\n",
    "bin_train_data = binarize(train_data)\n",
    "assert bin_train_data.dtype == np.float\n",
    "assert bin_train_data.shape == train_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0a39404cc2f67078b399ee34653a3ac",
     "grade": false,
     "grade_id": "cell-462e747685e8670f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Sample a few images of digits $2$, $3$ and $4$; and show both the original and the binarized image together with their label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f3c981f0fda5ba3bdfcefb9144305c7",
     "grade": true,
     "grade_id": "cell-784c6bd177a9aa42",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaMAAABwCAYAAACzSB/BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABMxJREFUeJzt3bFrFVsQB+DZIIKNQUWCIEgKtdPKQgsjRETsU0XsVPDfSCVYp0phYUqJnYVVqkTUUk0UBEULO0EwCMJavZN9zyRe7kt2TrjfVw1LzJ1qf8x47knTtm0AQKax7AYAQBgBkE4YAZBOGAGQThgBkE4YAZBOGAGQThgBkE4YAZDuQJ8f1jSN6x6G0LZtk90DjDrvr+EM+v4yGQGQThgBkE4YAZBOGAGQThgBkE4YAZBOGAGQThgBkE4YAZBOGAGQThgBkE4YAZBOGAGQrtdbu0dF225e7ts0LtwG6jc9PR0REYuLi+XZ1NRUqdfX1/f0801GAKQTRgCks6YD2GWXL18u9bFjx0q9tLSU0c5ALly4EBERL168SPl8kxEA6fbFZLRfDgR0+wRG15UrV0p9+vTpUtc2GY2Nbc4jk5OTERFx6tSp8qzP963JCIB0wgiAdNWu6ay8gP3q1q1bpV5ZWUnsZGcnTpwo9e3btyMi4tGjR+XZ2tpab72YjABIJ4wASFftmm6/2GqdWPOJP2DvdU+p1WxhYeGPZ+/fv0/oxGQEQAWEEQDprOkAdsG5c+dKPTExkdjJ4MbHx/949uzZs4ROTEYAVEAYAZDOmg5gF9y4caPUhw4dSuxkZ90V4j/30XV9+fKlz3YKkxEA6aqajLa7Aqi27+24qgj4r7Nnz275/PXr1z13srMHDx6UujslvXv3LiIivn//3ntPESYjACogjABIV9WarmaDrOZqWycC+fr+M96HDx8u9fXr1yMi4ubNm+XZtWvXtvx3c3NzERHx7du3PexueyYjANIJIwDSjdyazkk4oE9Hjx4d+GfPnz9f6u7a/+rVqxERcfLkyfLs4MGDpZ6dnS1198bwjY2NiIh4/vx5efbz589SHziwGQGvXr0auM+9YDICIJ0wAiBd0+faqmmaHT+slhXa307F9f3l3LZtHdODZH97f83Pz5f67t27pe6eTvv06dOOn9G9+bv7Pvn161dERPz48aM8e/PmTam7a7iXL1+Wenl5OSIivn79Wp59/vy51EeOHCl1d+23mwZ9f5mMAEgnjABIV9Vpuu3WXMOu73wJFejLvXv3Sv3x48dSX7p0aeDf0V3jPXnypNRv376NiIjV1dWhertz506pjx8/XuoPHz4M9fv2gskIgHRVTUbbqWHCqeVwBVC/+/fvZ7fwL9PT01s+f/z4cc+dbM9kBEA6YQRAun2xpqtZDStEgGEsLS1lt1CYjABIJ4wASCeMAEgnjABIJ4wASOc0HcAI6Z4APnPmTKmHvWpot5iMAEhnMgIYId2rzbp/ojxbPZ0AMLKEEQDprOn+p+7I62ogYD+5ePFiqR8+fJjXSJiMAKiAMAIgnTUdwAip9b8TTEYApBNGAKSzphtQd7TtnqADqN3Tp09LPTMzk9jJ9kxGAKQTRgCka/pcOTVNY781hLZt6zz+AiPE+2s4g76/TEYApBNGAKQTRgCkE0YApBNGAKQTRgCkE0YApOv1e0YAsBWTEQDphBEA6YQRAOmEEQDphBEA6YQRAOmEEQDphBEA6YQRAOmEEQDphBEA6YQRAOmEEQDphBEA6YQRAOmEEQDphBEA6YQRAOmEEQDphBEA6YQRAOmEEQDphBEA6X4DZFra9c7XRs4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_dict = {}\n",
    "for i in range(10):\n",
    "    index_dict[i] = np.where(train_labels == i)[0]\n",
    "    \n",
    "plot_number = 0  \n",
    "for i in [2, 3, 4]:\n",
    "    for j in range(2):\n",
    "        plt.figure(figsize=(10,10))\n",
    "        \n",
    "        #binairy image\n",
    "        plot_number += 1\n",
    "        subplot(6, 2, plot_number)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(bin_train_data[index_dict[i][j]].reshape((28, 28)), cmap='gray')\n",
    "        \n",
    "        #original image\n",
    "        plot_number += 1\n",
    "        subplot(6, 2, plot_number)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(train_data[index_dict[i][j]].reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b9da574d24193df76e96ed8ca62c7b0",
     "grade": false,
     "grade_id": "cell-56b33654497d4052",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 1.2 Implementation (40 points)\n",
    "You are going to write a function ```EM(X, K, max_iter)``` that implements the EM algorithm on the Bernoulli mixture model. \n",
    "\n",
    "The only parameters the function has are:\n",
    "* ```X``` :: (NxD) array of input training images\n",
    "* ```K``` :: size of the latent space\n",
    "* ```max_iter``` :: maximum number of iterations, i.e. one E-step and one M-step\n",
    "\n",
    "You are free to specify your return statement.\n",
    "\n",
    "Make sure you use a sensible way of terminating the iteration process early to prevent unnecessarily running through all epochs. Vectorize computations using ```numpy``` as  much as possible.\n",
    "\n",
    "You should implement the `E_step(X, mu, pi)` and `M_step(X, gamma)` separately in the functions defined below. These you can then use in your function `EM(X, K, max_iter)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "316c9131692747c363b5db8e9091d362",
     "grade": false,
     "grade_id": "cell-882b13c117a73cc4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def E_step(X, mu, pi):\n",
    "    N = X.shape[0]\n",
    "    K = pi.shape[0]\n",
    "    gamma = np.zeros((N,K))\n",
    "    \n",
    "    for n in range(N):    \n",
    "        # calculate denominator\n",
    "        normalizing = 0\n",
    "        for k in range(K):\n",
    "            \n",
    "            # calculate responsibility by factorizing over D\n",
    "            gamma[n,k] = pi[k] * np.prod(mu[k]**X[n] * (1-mu[k])**(1-X[n]))\n",
    "            # find normalizing constant\n",
    "            normalizing += gamma[n,k]\n",
    "            \n",
    "        gamma[n] /= normalizing\n",
    "\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1418f4014e98024fc97446ce27766c1d",
     "grade": true,
     "grade_id": "cell-f7c7dd52d82e2498",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's test on 5 datapoints\n",
    "n_test = 5\n",
    "X_test = bin_train_data[:n_test]\n",
    "D_test, K_test = X_test.shape[1], 10\n",
    "\n",
    "np.random.seed(2018)\n",
    "mu_test = np.random.uniform(low=.25, high=.75, size=(K_test,D_test))\n",
    "pi_test = np.ones(K_test) / K_test\n",
    "\n",
    "gamma_test = E_step(X_test, mu_test, pi_test)\n",
    "assert gamma_test.shape == (n_test, K_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c426a613653174795cd9c8327ab6e20",
     "grade": false,
     "grade_id": "cell-f1b11b8765bd1ef6",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def M_step(X, gamma):\n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    K = gamma.shape[1]\n",
    "    \n",
    "    mu = np.zeros((K, D))\n",
    "    pi = np.zeros((K))\n",
    "    \n",
    "    for k in range(K):\n",
    "        mu_k = 0\n",
    "        N_k = 0\n",
    "        for n in range(N):\n",
    "            mu_k += gamma[n, k] * X[n, :]\n",
    "            N_k += gamma[n, k]\n",
    "        mu_k = mu_k / N_k\n",
    "        mu[k, :] = mu_k\n",
    "        pi[k] = N_k / N    \n",
    "        \n",
    "    \n",
    "    return mu, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f60d48b8b22063cef560b42944a0aa4",
     "grade": true,
     "grade_id": "cell-6e7c751b30acfd45",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Oh, let's test again\n",
    "mu_test, pi_test = M_step(X_test, gamma_test)\n",
    "\n",
    "assert mu_test.shape == (K_test,D_test)\n",
    "assert pi_test.shape == (K_test, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "acfec6384b058cb0ce1932006fbfebc4",
     "grade": true,
     "grade_id": "cell-d6c4368246dee7e6",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def EM(X, K, max_iter, mu=None, pi=None):\n",
    "    eps = 10e-7\n",
    "    if mu is None:\n",
    "#         mu = 0.5*np.ones((K,X.shape[1]))\n",
    "        mu = np.random.uniform(low=.25, high=.75, size=(K, X.shape[1]))\n",
    "\n",
    "    if pi is None:\n",
    "        pi = np.ones((K)) / K\n",
    "    \n",
    "    old_log_likelihood = -inf\n",
    "    eps = 10e-5\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "        gamma_new = E_step(X, mu, pi)\n",
    "        mu_new, pi_new = M_step(X, gamma_new)\n",
    "\n",
    "        # calculate likelihood\n",
    "        # add eps so log of zero does not result in NaN\n",
    "        log_x_mu = X @ np.log(mu_new.T + eps) + (1.0-X) @ np.log(1-mu_new.T + eps)\n",
    "        log_likelihood = np.sum(gamma_new * (np.log(pi_new + eps) + log_x_mu))\n",
    "        \n",
    "        # check for convergence\n",
    "        if log_likelihood - old_log_likelihood > eps:\n",
    "            old_log_likelihood = log_likelihood\n",
    "            mu, pi, gamma = mu_new, pi_new, gamma_new\n",
    "        else:\n",
    "            print(f\"Converged after {i} iterations\")\n",
    "            break\n",
    "        \n",
    "    return mu, pi, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4fc12faa0da660f7a4d9cc7deb41b25",
     "grade": false,
     "grade_id": "cell-e1077ed3b83489be",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 1.3 Three digits experiment (10 points)\n",
    "In analogue with Bishop $\\S9.3.3$, sample a training set consisting of only __binary__ images of written digits $2$, $3$, and $4$. Run your EM algorithm and show the reconstructed digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdbce0fad0ed151063d4c489ce999e3e",
     "grade": true,
     "grade_id": "cell-477155d0264d7259",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 10\n",
      "Iteration 20\n",
      "Converged after 28 iterations\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEOdJREFUeJzt3VuM3dV1x/HfwuDbjG08vsyM7QFjwOUmQZAFlagQVUREq0iQh6DwULlqFOchSI3UhyJeglRFQlWTNk+RHGHFSAlJJKDwELWJICqtVFnYyIpNKLYBN5nO4PGdwTY2tlcf5tAOMP+1xuf2PzP7+5GsmTlr/udsH8/P/3Nm/ffe5u4CUJ6r6h4AgHoQfqBQhB8oFOEHCkX4gUIRfqBQhB8oFOEHCkX4gUJd3c0HMzMuJwQ6zN1tNt/X0pnfzB4ys7fN7JCZPdHKfQHoLmv22n4zWyDpgKQHJY1Kel3SY+7+u+AYzvxAh3XjzH+PpEPu/q67X5D0M0kPt3B/ALqolfCvl/SHaV+PNm77FDPbZma7zWx3C48FoM1a+YXfTC8tPvey3t23S9ou8bIf6CWtnPlHJY1M+3qDpLHWhgOgW1oJ/+uSbjazG8xsoaSvSXq5PcMC0GlNv+x394tm9rikf5W0QNIOd3+zbSMD0FFNt/qaejDe8wMd15WLfADMXYQfKBThBwpF+IFCEX6gUIQfKBThBwpF+IFCEX6gUIQfKBThBwpF+IFCEX6gUIQfKBThBwpF+IFCEX6gUIQfKBThBwpF+IFCEX6gUF3doruXmcULni5YsKCy1t/fHx67cOHCsD4wMBDWh4eHw/qyZcsqaytWrAiPvfrq+EcgW9356NGjYf3IkSOVtZMnT4bHnjp1Kqx/+OGHYf3SpUuVtYsXL4bHloAzP1Aowg8UivADhSL8QKEIP1Aowg8UivADhWqpz29mhyVNSrok6aK7b2nHoOqwaNGisL5y5crK2sjISHjsbbfdFtbvvffesL558+awvmbNmsra4OBgeGx2jcKJEyfC+sTERFh/7733Kmt79uwJj927d29YP3jwYFiPriPIrhG4cOFCWM90c/frZrXjIp8/dfdjbbgfAF3Ey36gUK2G3yX9ysz2mNm2dgwIQHe0+rL/PncfM7O1kn5tZv/l7q9N/4bGfwr8xwD0mJbO/O4+1vg4IelFSffM8D3b3X3LXP5lIDAfNR1+M+szs2WffC7pS5L2t2tgADqrlZf9g5JebEyFvVrST939X9oyKgAd13T43f1dSXe2cSwtyebjZ3Pqs3nv119/fWXtzjvjp+H+++8P67fccktYj/r4Uv53i2TXN2zYsCGsL1++PKxH10esXr06PHbdunVh/dVXXw3r+/btq6xdvnw5PDab75/18edCn59WH1Aowg8UivADhSL8QKEIP1Aowg8Uat4s3Z21+jJZ2ylaHnvp0qXhsWfPng3r0fLWs6lPTk5W1rLlr7PnbWhoKKxHz4sk9fX1Vdauueaa8NjFixeH9VWrVoX1aOzRcyblz0vWKpwLOPMDhSL8QKEIP1Aowg8UivADhSL8QKEIP1CoedPnz6ZQZltRZ0s5nzt3rrJ2+PDh8NiPPvoorO/atSusnz59OqxH1wGcP38+PDbrpWfbg999991hPZquHF0DIElXXRWfm5YsWRLWP/7448paK1uyS/Nji2/O/EChCD9QKMIPFIrwA4Ui/EChCD9QKMIPFKqYPn/Wa8/m3B87Vr0R8dGjR8NjDxw4ENajawikuF8tSZcuXaqsZfPOBwYGwnq2NHf02FLcT8+uvch68dm1GWfOnKmsZdc/ZM/5fMCZHygU4QcKRfiBQhF+oFCEHygU4QcKRfiBQqV9fjPbIenLkibc/Y7GbQOSfi5po6TDkh5195OdG2brsn501jOO+r5ZLz1bnz67RiG7/2iL7rVr14bH3nDDDWH91ltvDevZFt4bN26srJ08Gf/IZHsOHD9+PKxH109cuHAhPLYEsznz/1jSQ5+57QlJr7j7zZJeaXwNYA5Jw+/ur0k68ZmbH5a0s/H5TkmPtHlcADqs2ff8g+4+LkmNj/FrSwA9p+PX9pvZNknbOv04AK5Ms2f+I2Y2LEmNjxNV3+ju2919i7tvafKxAHRAs+F/WdLWxudbJb3UnuEA6JY0/Gb2nKT/lPRHZjZqZl+X9LSkB83soKQHG18DmEPS9/zu/lhF6YttHkutsr5vtE57tsZ7Ni89m9ee9dKjtfWzY9evXx/Wsz5/tC6/FD832ToG4+PjYX1ycrLpevbvnV1bMR9whR9QKMIPFIrwA4Ui/EChCD9QKMIPFGreLN3dadG022wr6UWLFoX1devWhfWhoaGwfvvtt1fWNm3aFB573XXXhfVoSq4kLV26NKxH7bZsSm82zbqVZcOzf7OsPZtNw54LOPMDhSL8QKEIP1Aowg8UivADhSL8QKEIP1Ao+vwNWc84mpqa9YyXLVsW1lesWBHWs+W1b7rppspa1sfP6v39/WE9Ey15nm2LvmrVqpbq0fOebdme/TxEU7znCs78QKEIP1Aowg8UivADhSL8QKEIP1Aowg8Uij7/LEV936zne/78+bCezR3Plpk+duxYZS3bHjxbdjxbwjq7xuH06dOVtex5y8aerVXQyhbd2b8ZfX4AcxbhBwpF+IFCEX6gUIQfKBThBwpF+IFCpX1+M9sh6cuSJtz9jsZtT0n6hqSjjW970t1/2alB9oKoFx/NWZfyeeujo6NhPbv/EydOVNayNf8PHToU1rPtw7M9CaL17bPrG7I+/5IlS8J6tP34xMREeGw2338+bPE9mzP/jyU9NMPt/+judzX+zOvgA/NRGn53f01S9akFwJzUynv+x83st2a2w8xWtm1EALqi2fD/UNKNku6SNC7pe1XfaGbbzGy3me1u8rEAdEBT4Xf3I+5+yd0vS/qRpHuC793u7lvcfUuzgwTQfk2F38yGp335FUn72zMcAN0ym1bfc5IekLTazEYlfUfSA2Z2lySXdFjSNzs4RgAdkIbf3R+b4eZnOjCWjmp1v/Vo3np231nPOOrTz+b4aK2BbI/7vr6+sJ5dY5CJ1v1fvHhxeOzmzZvDetbnjx472ythbGwsrLe6DkIv4Ao/oFCEHygU4QcKRfiBQhF+oFCEHyjUvFm6O1tCutUlrKN2WtYmzOrZ9NDly5c3fXy0fPVs6lmbMWtpRe28bHvwVu5bkhYuXFhZy6YiZ1uTT05OhvVWW6TdwJkfKBThBwpF+IFCEX6gUIQfKBThBwpF+IFCzZs+fytTcqX8OoCo75s9diabmpr1+aO/W7aVdNbHz+qZ6HnL/l7XXnttWM/+zVrZujz7ecmWNG/lupFu4cwPFIrwA4Ui/EChCD9QKMIPFIrwA4Ui/ECh5k2fP5P1bbOec9QXznrGWX3p0qVhfeXKeCvEqJefzYnPlvbOZEt/R3P2N27cGB47PDwc1rN1EKIl1aO5/rOpZ33+rE6fH0BtCD9QKMIPFIrwA4Ui/EChCD9QKMIPFCrt85vZiKRnJQ1Juixpu7v/wMwGJP1c0kZJhyU96u4nOzfUeI50tk12Vs/WcR8cHKysZT3hoaGhsJ5dgzAwMBDWo79bNh9/7dq1YT3rpY+MjIT1G2+8sbK2adOm8NhVq1aF9ePHj4f1qNeeXf+QrcvfC336Vs3mzH9R0t+4+62S/ljSt8zsNklPSHrF3W+W9ErjawBzRBp+dx939zcan09KekvSekkPS9rZ+Ladkh7p1CABtN8Vvec3s42SviBpl6RBdx+Xpv6DkBS/fgTQU2Z9bb+Z9Ut6XtK33f2D7D30tOO2SdrW3PAAdMqszvxmdo2mgv8Td3+hcfMRMxtu1IclTcx0rLtvd/ct7r6lHQMG0B5p+G3qFP+MpLfc/fvTSi9L2tr4fKukl9o/PACdMpuX/fdJ+gtJ+8xsb+O2JyU9LekXZvZ1Sb+X9NXODPH/RUtkZ+2yVrfRjtqMGzZsCI/NWlpRO0zK21LRW7DsecnuO5vqHLVAJWnNmjWVtWwq8/vvvx/WR0dHw/r4+Hhl7Z133gmPzZY8P3/+fEvH94I0/O7+H5Kqfrq+2N7hAOgWrvADCkX4gUIRfqBQhB8oFOEHCkX4gULNqaW7o550tjx21sc/e/Zs04+9ePHi8NjVq1eH9RUrVoT1aPlrKV7iOtv+O7sOIJu6mm1FffTo0cragQMHwmP37NkT1t98882wvm/fvqbGJUmnTp0K69lU6Va3be8GzvxAoQg/UCjCDxSK8AOFIvxAoQg/UCjCDxRqTvX5I9kS09ny2ufOnQvrY2NjlbWsV97f3x/Ws+2cs/uPeu3ZfPysH531sw8dOhTWDx48WFl7++23w2P3798f1rP5/BMTMy4uJSmfj5+tczAfcOYHCkX4gUIRfqBQhB8oFOEHCkX4gUIRfqBQ1s15x2bWs5OcW+mlZ1toZ33+vr6+sL5+/fqwvmzZsspa9vfK+t1nzpwJ6x988EFYj+bNZ8dmayxkYyuVu89qLz3O/EChCD9QKMIPFIrwA4Ui/EChCD9QKMIPFCrt85vZiKRnJQ1Juixpu7v/wMyekvQNSZ80cp90918m99Wzff6MWXXrNFu7PjpWan2N92jueQnz0vFps+3zzyb8w5KG3f0NM1smaY+kRyQ9KulDd/+H2Q6K8M+M8KOdZhv+dCUfdx+XNN74fNLM3pIUX3IGoOdd0Xt+M9so6QuSdjVuetzMfmtmO8xsZcUx28xst5ntbmmkANpq1tf2m1m/pH+T9F13f8HMBiUdk+SS/k5Tbw3+KrkPXvbPgJf9aKe2XttvZtdIel7ST9z9hcYDHHH3S+5+WdKPJN3T7GABdF8afps6bT0j6S13//6026dvDfsVSfFSqwB6ymx+2/8nkv5d0j5Ntfok6UlJj0m6S1Mv+w9L+mbjl4PRfc3Zl/3AXNG2Vl87EX6g85jPDyBE+IFCEX6gUIQfKBThBwpF+IFCEX6gUIQfKBThBwpF+IFCEX6gUIQfKBThBwpF+IFCpQt4ttkxSf897evVjdt6Ua+OrVfHJTG2ZrVzbNfP9hu7Op//cw9uttvdt9Q2gECvjq1XxyUxtmbVNTZe9gOFIvxAoeoO//aaHz/Sq2Pr1XFJjK1ZtYyt1vf8AOpT95kfQE1qCb+ZPWRmb5vZITN7oo4xVDGzw2a2z8z21r3FWGMbtAkz2z/ttgEz+7WZHWx8nHGbtJrG9pSZ/U/judtrZn9e09hGzOw3ZvaWmb1pZn/duL3W5y4YVy3PW9df9pvZAkkHJD0oaVTS65Iec/ffdXUgFczssKQt7l57T9jM7pf0oaRn3f2Oxm1/L+mEuz/d+I9zpbv/bY+M7Sld4c7NHRpb1c7Sf6kan7t27njdDnWc+e+RdMjd33X3C5J+JunhGsbR89z9NUknPnPzw5J2Nj7fqakfnq6rGFtPcPdxd3+j8fmkpE92lq71uQvGVYs6wr9e0h+mfT2q3try2yX9ysz2mNm2ugczg8FPdkZqfFxb83g+K925uZs+s7N0zzx3zex43W51hH+m3UR6qeVwn7vfLenPJH2r8fIWs/NDSTdqahu3cUnfq3MwjZ2ln5f0bXf/oM6xTDfDuGp53uoI/6ikkWlfb5A0VsM4ZuTuY42PE5JeVO/tPnzkk01SGx8nah7P/+mlnZtn2llaPfDc9dKO13WE/3VJN5vZDWa2UNLXJL1cwzg+x8z6Gr+IkZn1SfqSem/34ZclbW18vlXSSzWO5VN6Zefmqp2lVfNz12s7XtdykU+jlfFPkhZI2uHu3+36IGZgZps0dbaXpmY8/rTOsZnZc5Ie0NSsryOSviPpnyX9QtJ1kn4v6avu3vVfvFWM7QFd4c7NHRpb1c7Su1Tjc9fOHa/bMh6u8APKxBV+QKEIP1Aowg8UivADhSL8QKEIP1Aowg8UivADhfpfJGKUhtGy3o8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "indexes_234 = list(index_dict[2]) + list(index_dict[3]) + list(index_dict[4])\n",
    "indexes_sample = sample(indexes_234, 1000)\n",
    "X_234 = array([bin_train_data[i] for i in indexes_sample])\n",
    "\n",
    "iterations = 1000\n",
    "mu, pi, gamma = EM(X_234, 3, iterations)\n",
    "\n",
    "for k in range(mu.shape[0]):\n",
    "    plt.figure()\n",
    "    plt.imshow(mu[k].reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "485543f4893938d2a9dc1c17d8221cbc",
     "grade": false,
     "grade_id": "cell-88c9664f995b1909",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Can you identify which element in the latent space corresponds to which digit? What are the identified mixing coefficients for digits $2$, $3$ and $4$, and how do these compare to the true ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae7b5acea6089e2590059f90b0d0a0be",
     "grade": true,
     "grade_id": "cell-3680ae2159c48193",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98e04feb59a36867367b3027df9e226d",
     "grade": false,
     "grade_id": "cell-0891dda1c3e80e9a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 1.4 Experiments (20 points)\n",
    "Perform the follow-up experiments listed below using your implementation of the EM algorithm. For each of these, describe/comment on the obtained results and give an explanation. You may still use your dataset with only digits 2, 3 and 4 as otherwise computations can take very long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "439067186fa3ef1d7261a9bcf5a84ea6",
     "grade": false,
     "grade_id": "cell-06fe1b1355689928",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### 1.4.1 Size of the latent space (5 points)\n",
    "Run EM with $K$ larger or smaller than the true number of classes. Describe your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "791512aeadd30c4b586b966ca10e6fad",
     "grade": true,
     "grade_id": "cell-6c9057f2546b7215",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 10\n",
      "Iteration 20\n",
      "Converged after 21 iterations\n",
      "Iteration 0\n",
      "Iteration 10\n",
      "Converged after 19 iterations\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = 1000\n",
    "\n",
    "\n",
    "mu, pi, gamma = EM(X_234, 2, iterations)\n",
    "\n",
    "for k in range(mu.shape[0]):\n",
    "    plt.figure()\n",
    "    plt.imshow(mu[k].reshape((28, 28)), cmap='gray')\n",
    "    \n",
    "    \n",
    "mu, pi, gamma = EM(X_234, 4, iterations)\n",
    "\n",
    "for k in range(mu.shape[0]):\n",
    "    plt.figure()\n",
    "    plt.imshow(mu[k].reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e12e40c2d2165e3bb500b5504128910d",
     "grade": true,
     "grade_id": "cell-f01c37653160244b",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "When K = 2 (in stead of 3), the reconstructions are less clear and a bit blurred. Although the number '3' and '4' seem to be dominant over the number '2'.\n",
    "When K = 4 (in stead of 3) the images are fairly sharper and also, the EM algorithm needs fewer steps until convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b306681523a2e35eea310ac10bb68999",
     "grade": false,
     "grade_id": "cell-cf478d67239b7f2e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### 1.4.2 Identify misclassifications (10 points)\n",
    "How can you use the data labels to assign a label to each of the clusters/latent variables? Use this to identify images that are 'misclassified' and try to understand why they are. Report your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "000c11bd8756a4e24296c7c55d3ee17e",
     "grade": true,
     "grade_id": "cell-daa1a492fbba5c7e",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 10\n",
      "Iteration 20\n",
      "Converged after 20 iterations\n"
     ]
    }
   ],
   "source": [
    "mu, pi, gamma = EM(X_234, 3, iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990 misclassified images\n"
     ]
    }
   ],
   "source": [
    "misclassified = 0\n",
    "for idx in range(len(X_234)):\n",
    "    predicted_label = np.argmax(gamma[idx,:])\n",
    "    true_label = train_labels[indexes_sample[idx]]\n",
    "    if predicted_label != true_label:\n",
    "        # show misclassified image for check\n",
    "#         plt.imshow(X_234[idx].reshape((28, 28)), cmap='gray')\n",
    "        misclassified+=1\n",
    "\n",
    "print(f\"{misclassified} misclassified images\")\n",
    "\n",
    "# predictions = []\n",
    "# for i in range(len(data)):\n",
    "#         predictions[i] = assigned_classes[np.argmax(gamma[i, :])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "baf43434481c13d76ad51e3ba07e2bf5",
     "grade": true,
     "grade_id": "cell-329245c02df7850d",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "640bc57a2d08c3becf534bb5e4b35971",
     "grade": false,
     "grade_id": "cell-67ce1222e8a7837b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### 1.4.3 Initialize with true values (5 points)\n",
    "Initialize the three classes with the true values of the parameters and see what happens. Report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a48f788e286458ef0f776865a3bcd58b",
     "grade": true,
     "grade_id": "cell-aa5d6b9f941d985d",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33333333 0.33333333 0.33333333]\n",
      "(1000, 784)\n",
      "(784,)\n"
     ]
    }
   ],
   "source": [
    "# Klopt nog niet helemaal\n",
    "K = 3\n",
    "pi_true = np.ones(K) * 1 / K\n",
    "mu_true = np.mean(X_234, axis=0)\n",
    "print(pi_true)\n",
    "print(X_234.shape)\n",
    "print(mu_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1dc4adf3081f3bec93f94c3b12b87db9",
     "grade": true,
     "grade_id": "cell-981e44f35a3764b0",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd613f41e5d2b7d22b0d5b1e7644a48a",
     "grade": false,
     "grade_id": "cell-19bfd7cf4017ed84",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Part 2: Variational Auto-Encoder\n",
    "\n",
    "A Variational Auto-Encoder (VAE) is a probabilistic model $p(\\bx, \\bz)$ over observed variables $\\bx$ and latent variables and/or parameters $\\bz$. Here we distinguish the decoder part, $p(\\bx | \\bz) p(\\bz)$ and an encoder part $p(\\bz | \\bx)$ that are both specified with a neural network. A lower bound on the log marginal likelihood $\\log p(\\bx)$ can be obtained by approximately inferring the latent variables z from the observed data x using an encoder distribution $q(\\bz| \\bx)$ that is also specified as a neural network. This lower bound is then optimized to fit the model to the data. \n",
    "\n",
    "The model was introduced by Diederik Kingma (during his PhD at the UVA) and Max Welling in 2013, https://arxiv.org/abs/1312.6114. \n",
    "\n",
    "Since it is such an important model there are plenty of well written tutorials that should help you with the assignment. E.g: https://jaan.io/what-is-variational-autoencoder-vae-tutorial/.\n",
    "\n",
    "In the following, we will make heavily use of the torch module, https://pytorch.org/docs/stable/index.html. Most of the time replacing `np.` with `torch.` will do the trick, e.g. `np.sum` becomes `torch.sum` and `np.log` becomes `torch.log`. In addition, we will use `torch.FloatTensor()` as an equivalent to `np.array()`. In order to train our VAE efficiently we will make use of batching. The number of data points in a batch will become the first dimension of our data tensor, e.g. A batch of 128 MNIST images has the dimensions [128, 1, 28, 28]. To check check the dimensions of a tensor you can call `.size()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92bd337f41c3f94777f47376c7149ca7",
     "grade": false,
     "grade_id": "cell-bcbe35b20c1007d3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.1 Loss function\n",
    "The objective function (variational lower bound), that we will use to train the VAE, consists of two terms: a log Bernoulli loss (reconstruction loss) and a Kullback–Leibler divergence. We implement the two terms separately and combine them in the end.\n",
    "As seen in Part 1: Expectation Maximization, we can use a multivariate Bernoulli distribution to model the likelihood $p(\\bx | \\bz)$ of black and white images. Formally, the variational lower bound is maximized but in PyTorch we are always minimizing therefore we need to calculate the negative log Bernoulli loss and Kullback–Leibler divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fb5f70b132e1233983ef89d19998374",
     "grade": false,
     "grade_id": "cell-389d81024af846e5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.1.1 Negative Log Bernoulli loss (5 points)\n",
    "The negative log Bernoulli loss is defined as,\n",
    "\n",
    "\\begin{align}\n",
    "loss = - (\\sum_i^D \\bx_i \\log \\hat{\\bx_i} + (1 − \\bx_i) \\log(1 − \\hat{\\bx_i})).\n",
    "\\end{align}\n",
    "\n",
    "Write a function `log_bernoulli_loss` that takes a D dimensional vector `x`, its reconstruction `x_hat` and returns the negative log Bernoulli loss. Make sure that your function works for batches of arbitrary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "952435ca03f47ab67a7e88b8306fc9a0",
     "grade": false,
     "grade_id": "cell-1d504606d6f99145",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def log_bernoulli_loss(x_hat, x):\n",
    "    loss = -torch.sum(x * torch.log(x_hat) + (1-x) * torch.log(1-x_hat)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd2a490aa694507bd032e86d77fc0087",
     "grade": true,
     "grade_id": "cell-9666dad0b2a9f483",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli: 6.476332187652588\n",
      "Bernoulli: 6.476332187652588\n"
     ]
    }
   ],
   "source": [
    "### Test test test\n",
    "x_test = torch.FloatTensor([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 0.9, 0.9, 0.9]])\n",
    "x_hat_test = torch.FloatTensor([[0.11, 0.22, 0.33, 0.44], [0.55, 0.66, 0.77, 0.88], [0.99, 0.99, 0.99, 0.99]])\n",
    "\n",
    "assert log_bernoulli_loss(x_hat_test, x_test) > 0.0\n",
    "assert log_bernoulli_loss(x_hat_test, x_test) < 10.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b75b7a531ecc87bce57925c4da464ee",
     "grade": false,
     "grade_id": "cell-b3a7c02dee7aa505",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.1.2 Negative Kullback–Leibler divergence (10 Points)\n",
    "The variational lower bound (the objective to be maximized) contains a KL term $D_{KL}(q(\\bz)||p(\\bz))$ that can often be calculated analytically. In the VAE we assume $q = N(\\bz, \\mu, \\sigma^2I)$ and $p = N(\\bz, 0, I)$. Solve analytically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d01a7e7fe2dcf5f1c5fb955b85c8a04a",
     "grade": true,
     "grade_id": "cell-4cab10fd1a636858",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "328115c94a66e8aba0a62896e647c3ba",
     "grade": false,
     "grade_id": "cell-c49899cbf2a49362",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Write a function `KL_loss` that takes two J dimensional vectors `mu` and `logvar` and returns the negative Kullback–Leibler divergence. Where `logvar` is $\\log(\\sigma^2)$. Make sure that your function works for batches of arbitrary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33b14b79372dd0235d67bb66921cd3e0",
     "grade": false,
     "grade_id": "cell-125b41878005206b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def KL_loss(mu, logvar):\n",
    "    loss = -0.5 * torch.sum(1 + logvar - mu**2 - logvar.exp())\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf72e196d2b60827e8e940681ac50a07",
     "grade": true,
     "grade_id": "cell-ba714bbe270a3f39",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test test test\n",
    "mu_test = torch.FloatTensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
    "logvar_test = torch.FloatTensor([[0.01, 0.02], [0.03, 0.04], [0.05, 0.06]])\n",
    "\n",
    "assert KL_loss(mu_test, logvar_test) > 0.0\n",
    "assert KL_loss(mu_test, logvar_test) < 10.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65335a588baac26bc48dd6c4d275fdca",
     "grade": false,
     "grade_id": "cell-18cb3f8031edec23",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.1.3 Putting the losses together (5 points)\n",
    "Write a function `loss_function` that takes a D dimensional vector `x`, its reconstruction `x_hat`, two J dimensional vectors `mu` and `logvar` and returns the final loss. Make sure that your function works for batches of arbitrary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6ecb5b60b2c8d7b90070ed59320ee70",
     "grade": false,
     "grade_id": "cell-d2d18781683f1302",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(x_hat, x, mu, logvar):\n",
    "    loss_KL = KL_loss(mu, logvar)\n",
    "    loss_bernoulli = log_bernoulli_loss(x, x_hat)\n",
    "    return (loss_KL + loss_bernoulli)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "816e9508408bfcb2c7332b508d505081",
     "grade": true,
     "grade_id": "cell-57747988d29bbb5d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli: 4.603850364685059\n",
      "Bernoulli: 4.603850364685059\n"
     ]
    }
   ],
   "source": [
    "x_test = torch.FloatTensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n",
    "x_hat_test = torch.FloatTensor([[0.11, 0.22, 0.33], [0.44, 0.55, 0.66], [0.77, 0.88, 0.99]])\n",
    "mu_test = torch.FloatTensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
    "logvar_test = torch.FloatTensor([[0.01, 0.02], [0.03, 0.04], [0.05, 0.06]])\n",
    "\n",
    "assert loss_function(x_hat_test, x_test, mu_test, logvar_test) > 0.0\n",
    "assert loss_function(x_hat_test, x_test, mu_test, logvar_test) < 10.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4506e06ed44a0535140582277a528ba4",
     "grade": false,
     "grade_id": "cell-9e3ba708967fe918",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.2 The model\n",
    "Below you see a data structure for the VAE. The modell itself consists of two main parts the encoder (images $\\bx$ to latent variables $\\bz$) and the decoder (latent variables $\\bz$ to images $\\bx$). The encoder is using 3 fully-connected layers, whereas the decoder is using fully-connected layers. Right now the data structure is quite empty, step by step will update its functionality. For test purposes we will initialize a VAE for you. After the data structure is completed you will do the hyperparameter search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31eccf2f6600764e28eb4bc6c5634e49",
     "grade": false,
     "grade_id": "cell-e7d9dafee18f28a1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F \n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, fc1_dims, fc21_dims, fc22_dims, fc3_dims, fc4_dims):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(*fc1_dims)\n",
    "        self.fc21 = nn.Linear(*fc21_dims)\n",
    "        self.fc22 = nn.Linear(*fc22_dims)\n",
    "        self.fc3 = nn.Linear(*fc3_dims)\n",
    "        self.fc4 = nn.Linear(*fc4_dims)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # To be implemented\n",
    "        raise Exception('Method not implemented')\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # To be implemented\n",
    "        raise Exception('Method not implemented')\n",
    "\n",
    "    def decode(self, z):\n",
    "        # To be implemented\n",
    "        raise Exception('Method not implemented')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # To be implemented\n",
    "        raise Exception('Method not implemented')\n",
    "\n",
    "VAE_test = VAE(fc1_dims=(784, 4), fc21_dims=(4, 2), fc22_dims=(4, 2), fc3_dims=(2, 4), fc4_dims=(4, 784))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a2243397998b4f55c25dfd734f3e7e0",
     "grade": false,
     "grade_id": "cell-c4f9e841b8972a43",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.3 Encoding (10 points)\n",
    "Write a function `encode` that gets a vector `x` with 784 elements (flattened MNIST image) and returns `mu` and `logvar`. Your function should use three fully-connected layers (`self.fc1()`, `self.fc21()`, `self.fc22()`). First, you should use `self.fc1()` to embed `x`. Second, you should use `self.fc21()` and `self.fc22()` on the embedding of `x` to compute `mu` and `logvar` respectively. PyTorch comes with a variety of activation functions, the most common calls are `F.relu()`, `F.sigmoid()`, `F.tanh()`. Make sure that your function works for batches of arbitrary size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "628bcd88c611cf01e70f77854600199b",
     "grade": false,
     "grade_id": "cell-93cb75b98ae76569",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def encode(self, x):\n",
    "    \n",
    "    x = F.relu(self.fc1(x))\n",
    "    mu = self.fc21(x)\n",
    "    logvar = self.fc22(x)\n",
    "    return mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "370d930fa9f10f1d3a451f3805c04d88",
     "grade": true,
     "grade_id": "cell-9648960b73337a70",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test, test, test\n",
    "VAE.encode = encode\n",
    "\n",
    "x_test = torch.ones((5,784))\n",
    "mu_test, logvar_test = VAE_test.encode(x_test)\n",
    "\n",
    "assert np.allclose(mu_test.size(), [5, 2])\n",
    "assert np.allclose(logvar_test.size(), [5, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f597cc2b5ef941af282d7162297f865",
     "grade": false,
     "grade_id": "cell-581b4ed1996be868",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.4 Reparameterization (10 points)\n",
    "One of the major question that the VAE is answering, is 'how to take derivatives with respect to the parameters of a stochastic variable?', i.e. if we are given $\\bz$ that is drawn from a distribution $q(\\bz|\\bx)$, and we want to take derivatives. This step is necessary to be able to use gradient-based optimization algorithms like SGD.\n",
    "For some distributions, it is possible to reparameterize samples in a clever way, such that the stochasticity is independent of the parameters. We want our samples to deterministically depend on the parameters of the distribution. For example, in a normally-distributed variable with mean $\\mu$ and standard deviation $\\sigma$, we can sample from it like this:\n",
    "\n",
    "\\begin{align}\n",
    "\\bz = \\mu + \\sigma \\odot \\epsilon,\n",
    "\\end{align}\n",
    "\n",
    "where $\\odot$ is the element-wise multiplication and $\\epsilon$ is sampled from $N(0, I)$.\n",
    "\n",
    "\n",
    "Write a function `reparameterize` that takes two J dimensional vectors `mu` and `logvar`. It should return $\\bz = \\mu + \\sigma \\odot \\epsilon$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6331cb5dd23aaacbcf1a52cfecb1afaa",
     "grade": false,
     "grade_id": "cell-679aea8b2adf7ec4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def reparameterize(self, mu, logvar):\n",
    "        eps = torch.randn(mu.size(0), mu.size(1))\n",
    "        z = mu + torch.exp(logvar * 0.5) * eps\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38d4e047717ab334b262c8c177f0a420",
     "grade": true,
     "grade_id": "cell-fdd7b27a3d17f84e",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test, test, test\n",
    "VAE.reparameterize = reparameterize\n",
    "VAE_test.train()\n",
    "\n",
    "mu_test = torch.FloatTensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
    "logvar_test = torch.FloatTensor([[0.01, 0.02], [0.03, 0.04], [0.05, 0.06]])\n",
    "\n",
    "z_test = VAE_test.reparameterize(mu_test, logvar_test)\n",
    "\n",
    "assert np.allclose(z_test.size(), [3, 2])\n",
    "assert z_test[0][0] < 5.0\n",
    "assert z_test[0][0] > -5.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9241ab0eaf8366c37ad57072ce66f095",
     "grade": false,
     "grade_id": "cell-0be851f9f7f0a93e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.5 Decoding (10 points)\n",
    "Write a function `decode` that gets a vector `z` with J elements and returns a vector `x_hat` with 784 elements (flattened MNIST image). Your function should use two fully-connected layers (`self.fc3()`, `self.fc4()`). PyTorch comes with a variety of activation functions, the most common calls are `F.relu()`, `F.sigmoid()`, `F.tanh()`. Make sure that your function works for batches of arbitrary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8e833cfd7c54a9b67a38056d5d6cab8",
     "grade": false,
     "grade_id": "cell-bf92bb3878275a41",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def decode(self, z):\n",
    "    \n",
    "    z = F.relu(self.fc3(z))\n",
    "    x_hat = F.sigmoid(self.fc4(z))\n",
    "    return x_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7732293fd7d971fcf255496e8c68638d",
     "grade": true,
     "grade_id": "cell-4abb91cb9e80af5d",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# test test test\n",
    "VAE.decode = decode\n",
    "\n",
    "z_test = torch.ones((5,2))\n",
    "x_hat_test = VAE_test.decode(z_test)\n",
    "\n",
    "assert np.allclose(x_hat_test.size(), [5, 784])\n",
    "assert (x_hat_test <= 1).all()\n",
    "assert (x_hat_test >= 0).all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2e113d1f45398b2a1399c336526e755",
     "grade": false,
     "grade_id": "cell-97511fbc4f5b469b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.6 Forward pass (10)\n",
    "To complete the data structure you have to define a forward pass through the VAE. A single forward pass consists of the encoding of an MNIST image $\\bx$ into latent space $\\bz$, the reparameterization of $\\bz$ and the decoding of $\\bz$ into an image $\\bx$.\n",
    "\n",
    "Write a function `forward` that gets a a vector `x` with 784 elements (flattened MNIST image) and returns a vector `x_hat` with 784 elements (flattened MNIST image), `mu` and `logvar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b7433c4631dd01c07a5fe287e55ae13",
     "grade": false,
     "grade_id": "cell-26bb463b9f98ebd5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    x = x.view(-1, 784)\n",
    "    \n",
    "    mu, logvar = self.encode(x)\n",
    "    z = self.reparameterize(mu, logvar)\n",
    "    x_hat = self.decode(z)\n",
    "    return x_hat, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e7e495f40465c162512e9873c360b25",
     "grade": true,
     "grade_id": "cell-347e5fba3d02754b",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# test test test \n",
    "VAE.forward = forward\n",
    "\n",
    "x_test = torch.ones((5,784))\n",
    "x_hat_test, mu_test, logvar_test = VAE_test.forward(x_test)\n",
    "\n",
    "assert np.allclose(x_hat_test.size(), [5, 784])\n",
    "assert np.allclose(mu_test.size(), [5, 2])\n",
    "assert np.allclose(logvar_test.size(), [5, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a114a6fd781fb949b887e6a028e07946",
     "grade": false,
     "grade_id": "cell-62c89e4d3b253671",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.7 Training (15)\n",
    "We will now train the VAE using an optimizer called Adam, https://arxiv.org/abs/1412.6980. The code to train a model in PyTorch is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3b6bb965fb48229c63cacda48baea65",
     "grade": false,
     "grade_id": "cell-be75f61b09f3b9b6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def train(epoch, train_loader, model, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data.view(-1, 784), mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48ca730dbef06a668f4dfdb24888f265",
     "grade": false,
     "grade_id": "cell-da1b063b7de850b9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let's train. You have to choose the hyperparameters. Make sure your loss is going down in a reasonable amount of epochs (around 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "846430258fb80f50b161135448726520",
     "grade": false,
     "grade_id": "cell-d4d4408d397f6967",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_dims = 256\n",
    "latent_dims = 2\n",
    "\n",
    "fc1_dims = (28*28,hidden_dims)\n",
    "fc21_dims = (hidden_dims, latent_dims)\n",
    "fc22_dims = (hidden_dims, latent_dims)\n",
    "fc3_dims = (latent_dims, hidden_dims)\n",
    "fc4_dims = (hidden_dims, 28*28)\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b93390f399b743276bc25e67493344f2",
     "grade": true,
     "grade_id": "cell-ca352d8389c1809a",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell contains a hidden test, please don't delete it, thx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20719070ed85964de9722acc3456a515",
     "grade": false,
     "grade_id": "cell-5c77370db7cec9f2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Run the box below to train the model using the hyperparameters you entered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38306be3638e85812bd5b2a052fcc0a4",
     "grade": false,
     "grade_id": "cell-5712d42de1068398",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli: inf\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: inf\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "====> Epoch: 1 Average loss: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "====> Epoch: 2 Average loss: nan\n",
      "Bernoulli: nan\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n",
      "Bernoulli: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-202-601b0aa70f1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVAE_MNIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-199-be1f04d67f7e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml2labs/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml2labs/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "# Load data\n",
    "train_data = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=batch_size, shuffle=True, **{})\n",
    "\n",
    "# Init model\n",
    "VAE_MNIST = VAE(fc1_dims=fc1_dims, fc21_dims=fc21_dims, fc22_dims=fc22_dims, fc3_dims=fc3_dims, fc4_dims=fc4_dims)\n",
    "\n",
    "# Init optimizer\n",
    "optimizer = optim.Adam(VAE_MNIST.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch, train_loader, VAE_MNIST, optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2f8fcc9384e30cb154cf931f223898b",
     "grade": false,
     "grade_id": "cell-bd07c058c661b9c6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Run the box below to check if the model you trained above is able to correctly reconstruct images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80d198e03b1287741d761a12e38dcf73",
     "grade": false,
     "grade_id": "cell-df03d717307a6863",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Let's check if the reconstructions make sense\n",
    "# Set model to test mode\n",
    "VAE_MNIST.eval()\n",
    "    \n",
    "# Reconstructed\n",
    "train_data_plot = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "\n",
    "train_loader_plot = torch.utils.data.DataLoader(train_data_plot,\n",
    "                                           batch_size=1, shuffle=False, **{})\n",
    "\n",
    "for batch_idx, (data, _) in enumerate(train_loader_plot):\n",
    "    x_hat, mu, logvar = VAE_MNIST(data)\n",
    "    plt.imshow(x_hat.view(1,28,28).squeeze().data.numpy(), cmap='gray')\n",
    "    plt.title('%i' % train_data.train_labels[batch_idx])\n",
    "    plt.show()\n",
    "    if batch_idx == 3:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f559122b150f5f1228d6b66b62f462c",
     "grade": false,
     "grade_id": "cell-76649d51fdf133dc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.8 Visualize latent space (20 points)\n",
    "Now, implement the auto-encoder now with a 2-dimensional latent space, and train again over the MNIST data. Make a visualization of the learned manifold by using a linearly spaced coordinate grid as input for the latent space, as seen in  https://arxiv.org/abs/1312.6114 Figure 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c879ffdb0d355349d7144a33d16ca93a",
     "grade": true,
     "grade_id": "cell-4a0af6d08d055bee",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9eb1684d646eea84a25638d184bfbda",
     "grade": false,
     "grade_id": "cell-dc5e1247a1e21009",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.8 Amortized inference (10 points)\n",
    "What is amortized inference? Where in the code of Part 2 is it used? What is the benefit of using it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "364ed922da59070f319d0bdfb0e41d92",
     "grade": true,
     "grade_id": "cell-6f7808a9b0098dbf",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
